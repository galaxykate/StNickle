{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some applications\n",
    "\n",
    "Here are a few of the experimental applications of phonetic similarity vectors included in the paper, including vector arithmetic, analogies, and sound symbolism tinting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use `spacy` for word probabilities. A \"reverse lookup\" based on phonetic similarity yields *all* of a particular transcription's homophones. Sometimes the first homophone returned is a weird word that no one ever uses. In order to maintain some semblance of readability, I sort all of the returned words sharing a pronunciation by their unigram probability and use the most probable word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, I create the Annoy index with pre-calculated vectors. The `words` contains all of the words, and the `lookup` dictionary maps a word to its index. I use these to look up words by their index in the Annoy index, and to find the Annoy index for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = AnnoyIndex(50, metric=\"euclidean\")\n",
    "words = list()\n",
    "lookup = dict()\n",
    "for i, line in enumerate(open(\"cmudict-0.7b-simvecs\", encoding=\"latin1\")):\n",
    "    word, vals_raw = line.split(\"  \")\n",
    "    word = word.lower().strip(\"(012)\")\n",
    "    vals = np.array([float(x) for x in vals_raw.split(\" \")])\n",
    "    t.add_item(i, vals)\n",
    "    words.append(word.lower())\n",
    "    lookup[word.lower()] = i\n",
    "t.build(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nnslookup` function takes an AnnoyIndex, a spaCy instance, a list of words, and a vector, and returns the most similar words by sound. It collates all homophones into groups and includes only the most common word for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roads', 'loads']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def nnslookup(t, nlp, words, vec, n=10):\n",
    "    res = t.get_nns_by_vector(vec, n)\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    last_vec = None\n",
    "    for item in res:\n",
    "        if last_vec is None or t.get_item_vector(item) == last_vec:\n",
    "            current_batch.append(item)\n",
    "            last_vec = t.get_item_vector(item)\n",
    "        else:\n",
    "            batches.append(current_batch[:])\n",
    "            current_batch = []\n",
    "            last_vec = None\n",
    "    if len(current_batch) > 0:\n",
    "        batches.append(current_batch[:])\n",
    "    output = []\n",
    "    for batch in batches:\n",
    "        output.append(sorted(batch, key=lambda x: nlp.vocab[words[x]].prob, reverse=True)[0])\n",
    "    return output\n",
    "[words[i] for i in nnslookup(t, nlp, words, t.get_item_vector(lookup[\"roads\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some arithmetic experiments\n",
    "\n",
    "### Averaging the sound of a sentence\n",
    "\n",
    "Find the word closest to the average of a sentence's phonetic similarity vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sitting in a room different from the one you are in now \n",
      "\t→ anacin, ison, anna, enlightening, enigma\n",
      "Double double toil and trouble fire burn and cauldron bubble \n",
      "\t→ untenable, dependable, unbuildable, detachable, tradable\n",
      "Peter Piper picked a peck of pickled peppers \n",
      "\t→ pipette, epileptic, pipetec, decapitated, epic\n",
      "Four score and seven years ago our fathers brought forth on this continent a new nation \n",
      "\t→ stelljes, uncoordinated, straightedge, coordinated, oriordan\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I am sitting in a room different from the one you are in now\",\n",
    "    \"Double double toil and trouble fire burn and cauldron bubble\",\n",
    "    \"Peter Piper picked a peck of pickled peppers\",\n",
    "    \"Four score and seven years ago our fathers brought forth on this continent a new nation\"\n",
    "]\n",
    "for s in sentences:\n",
    "    vecs = np.array([t.get_item_vector(lookup[w.lower()]) for w in s.split()])\n",
    "    mean = vecs.mean(axis=0)\n",
    "    print(s, \"\\n\\t→\", ', '.join([words[i] for i in nnslookup(t, nlp, words, mean, 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradual progress\n",
    "\n",
    "The `progress` function takes a list of vectors as a \"source\" (`src_vecs`) and gradually multiplies them by fractions of the vectors in a \"destination\" (`op_vecs`) over `n` steps, finding a gradual phonetic transition between the two lists of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(src_vecs, op_vecs, n=10):\n",
    "    for i in range(n+1):\n",
    "        delta = i * (1.0/n)\n",
    "        val = (src_vecs * (1.0-delta)) + (op_vecs * delta)\n",
    "        yield val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we gradually move from \"I am sitting in a room different from the one you are in now\" to a vector consisting entirely of the word \"buzzing\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sitting in a room different from the one you are in now\n",
      "i am sitting in a room different from the one you are in now\n",
      "i am sitting in a room different from the one you are in now\n",
      "i am sitting in a room different from the one you are in now\n",
      "i am sitting in a room different from the one hue are in now\n",
      "i am sitting in a room different schrum the one hue are in now\n",
      "i am sitting in a room different schrum the one hue are in now\n",
      "i am sitting in ah room different schrum the one hue are in now\n",
      "ah am sitting in ah room different schrum the one hue are in now\n",
      "ah am sitting in ah room different schrum the one hue are in now\n",
      "ah mam sitting in ah room different schrum the one hue are in now\n",
      "ah mam sitting inning ah rooming remittance schrum the one hue are inning romanow\n",
      "imai mam chickening inning imai rooming different schrum the one hue emminger inning romanow\n",
      "imai mam chickening inning imai rooming cinnaminson schrum the hyun ya emminger inning romanow\n",
      "imai mam committing inning imai rooming remittance schrum humming humming homonym emminger inning noiman\n",
      "imai mam humming inning imai rooming pinkham humming humming humming homonym emminger inning noiman\n",
      "imai humming humming humming imai rooming humming humming humming humming humming heminger humming numbing\n",
      "humming humming humming humming imai humming humming humming humming humming humming heminger humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n",
      "humming humming humming humming humming humming humming humming humming humming humming humming humming humming\n"
     ]
    }
   ],
   "source": [
    "s = \"I am sitting in a room different from the one you are in now\"\n",
    "vecs = np.array([t.get_item_vector(lookup[w.lower()]) for w in s.split()])\n",
    "print(s)\n",
    "op_vecs = np.array([t.get_item_vector(lookup[\"humming\"])]*len(vecs))\n",
    "for res in progress(vecs, op_vecs, n=25):\n",
    "    print(\" \".join([words[nnslookup(t, nlp, words, i)[0]] for i in res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the same thing in this next cell, moving from the slogan of my department (NYU ITP) to the word \"error\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perhaps the best way to describe us is as a Center for the Recently Possible\n",
      "perhaps the best way to describe us is as a center for the recently possible\n",
      "perhaps the best way to describe us is as a center for the recently possible\n",
      "perhaps the best way to describe us is as a center for the recently possible\n",
      "perhaps the best way to describe us is as a center for the recently possible\n",
      "perhaps the best whey to describe us is as a center for the recently possible\n",
      "perhaps the best whey to describe us is as ah center schreur the recently possible\n",
      "perhaps the best whey to describe us is as ah center schreur the recently possible\n",
      "perhaps the best whey to describe us is as ah center schreur the recently possible\n",
      "perhaps the best whey to describe us is as ah center schreur the recently possible\n",
      "perhaps the best erway to describe usair is as ah center schreur the erisa possible\n",
      "perhaps thy bestseller erway to ameridata usair is as ah center schreur thy erisa irrepressible\n",
      "inheritor thy bestseller airway to ameridata usair is as ah center schreur thy erisa irrepressible\n",
      "inheritor there're bestseller wearer atta aeritalia usair is as ah center schreur there're erisa irrepressible\n",
      "inheritor there're bearer wearer atta error usair error error ah error schreur there're erisa inheritable\n",
      "inheritor there're there're wearer error error error error error aigner error error there're erisa error\n",
      "inheritor there're error wearer error error error error error error error error there're erisa error\n",
      "error error error wearer error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n",
      "error error error error error error error error error error error error error error error\n"
     ]
    }
   ],
   "source": [
    "s = \"Perhaps the best way to describe us is as a Center for the Recently Possible\"\n",
    "vecs = np.array([t.get_item_vector(lookup[w.lower()]) for w in s.split()])\n",
    "print(s)\n",
    "op_vecs = np.array([t.get_item_vector(lookup[\"error\"])]*len(vecs))\n",
    "for res in progress(vecs, op_vecs, n=25):\n",
    "    print(\" \".join([words[nnslookup(t, nlp, words, i)[0]] for i in res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I find the latent letters in the alphabet by finding words \"between\" the way the letters are pronounced: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, eh, aah, b, beatie, beachy, c, teachey, deedee, g, d, e, eh,\n",
      "f, sffed, divi, vecci, g, dacey, tizzy, edgy, jaycee, h, aah, ai,\n",
      "i, ai, aah, ajay, j, che, 'kay, k, 'kay, cail, estai, ehle, l,\n",
      "ehle, mtel, m, airmen, en, n, en, aw, au, o, au, aw, ooh, p,\n",
      "kyowa, cue, q, cue, ru, ahr, r, ahr, 's, s, 's, essie, itchy,\n",
      "attie, t, ja, yoy, hew, ewe, u, ewe, hew, view, venue, v, buddie,\n",
      "dubay, w, extendable, aix, x, aix, equitex, ex-wife, why, iwai,\n",
      "why, wai, y, wai, why, zewe, xie, z\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "alpha = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "last = \"\"\n",
    "output = []\n",
    "for a, b in zip(alpha[:-1], alpha[1:]):\n",
    "    if a != last:\n",
    "        output.append(a)\n",
    "    last = a\n",
    "    for res in progress(np.array([t.get_item_vector(lookup[a])]), np.array([t.get_item_vector(lookup[b])]), n=30):\n",
    "        res = [words[t.get_nns_by_vector(i, n=1)[0]] for i in res][0]\n",
    "        if res != last:\n",
    "            output.append(res)\n",
    "        last = res\n",
    "    if b != last:\n",
    "        output.append(b)\n",
    "    last = b\n",
    "print(textwrap.fill(\", \".join(output), 65))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound analogies\n",
    "\n",
    "These analogies fill in this blank \"word A sounds like word B in the same way that word C sounds like word D.\" The `analogy()` function implements this with simple vector arithmetic (taking the difference of A and B and adding it to C to get D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analogy(t, w1, w2, w3):\n",
    "    vec = (np.array(t.get_item_vector(w2)) - np.array(t.get_item_vector(w1))) + np.array(t.get_item_vector(w3))\n",
    "    #return t.get_nns_by_vector(vec, 10)\n",
    "    return nnslookup(t, nlp, words, vec, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decide : decision :: explode : explosion\n",
      "final : finalize :: modern : modernize\n",
      "glory : glorify :: liquid : liquefied\n",
      "bite : bitten :: shake : shaken\n",
      "leaf : leaves :: calf : calves\n",
      "foot : feet :: tooth : keach\n",
      "automaton : automata :: criterion : criteria\n",
      "four : fourteen :: nine : nineteen\n",
      "light : slide :: lack : slag\n",
      "whisky : whimsy :: frisky : flimsy\n",
      "could : stood :: calling : stalling\n"
     ]
    }
   ],
   "source": [
    "good_groups = [\n",
    "    [\"decide\", \"decision\", \"explode\"], # explosion\n",
    "    [\"final\", \"finalize\", \"modern\"],\n",
    "    [\"glory\", \"glorify\", \"liquid\"],\n",
    "    [\"bite\", \"bitten\", \"shake\"], # shaken\n",
    "    [\"leaf\", \"leaves\", \"calf\"], # calves\n",
    "    [\"foot\", \"feet\", \"tooth\"], # teeth\n",
    "    [\"automaton\", \"automata\", \"criterion\"], # criteria\n",
    "    [\"four\", \"fourteen\", \"nine\"], # nineteen\n",
    "    [\"light\", \"slide\", \"lack\"], # slag\n",
    "    [\"whisky\", \"whimsy\", \"frisky\"], # flimsy\n",
    "    [\"could\", \"stood\", \"calling\"], # stalling\n",
    "]\n",
    "for w1, w2, w3 in good_groups:\n",
    "    # uncomment for latex table formatting\n",
    "    #print(\"%s & %s & %s & %s \\\\\\\\\" % (w1, w2, w3, words[analogy(t, lookup[w1], lookup[w2], lookup[w3])[0]]))\n",
    "    print(\"%s : %s :: %s : %s\" % (w1, w2, w3, words[analogy(t, lookup[w1], lookup[w2], lookup[w3])[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and subtracting sound\n",
    "\n",
    "The following functions add the vectors of two words together, or subtract one vector from another, and get the words phonetically closest to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vecsum(t, w1, w2, mult=1.0):\n",
    "    vec = (np.array(t.get_item_vector(w1)) + np.array(t.get_item_vector(w2))*mult)\n",
    "    return nnslookup(t, nlp, words, vec)\n",
    "def vecsub(t, w1, w2):\n",
    "    vec = (np.array(t.get_item_vector(w1)) - np.array(t.get_item_vector(w2)))\n",
    "    return nnslookup(t, nlp, words, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition (e.g., \"fizz\" + \"theology\" = \"physiology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eighteen', 'ate', 'it']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsum(t, lookup[\"ate\"], lookup[\"teen\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['missive', 'miss', 'missus', 'fis']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsum(t, lookup[\"miss\"], lookup[\"sieve\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['submarine', 'summarize', 'marine', 'submarines']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsum(t, lookup[\"sub\"], lookup[\"marine\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physiology', 'fizzle', 'sieve', 'sivy', 'feese']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsum(t, lookup[\"fizz\"], lookup[\"theology\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['snacking', 'qing', 'snaking', 'smacking', 'flanking']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsum(t, lookup[\"snack\"], lookup[\"king\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtraction (e.g., \"curiously\" - \"lee\" = \"curious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amerindian', \"burundi's\", 'uninspired', 'collaborating', 'emerine']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"submarine\"], lookup[\"sub\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dislodged', 'disregards', 'elsworth', 'discharged', 'aylesworth']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"wordsworth\"], lookup[\"word\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['javelin', 'raveling', 'televise', 'ravel', 'travelodge']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"lavender\"], lookup[\"under\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curious', 'kurian', 'curiosities', 'curatorial', 'murias']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"curiously\"], lookup[\"lee\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insignia', 'industrielle', 'argentinians', 'archaeologists', 'intracranial']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"ingredients\"], lookup[\"reed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authoritarianism',\n",
       " 'associating',\n",
       " 'professorial',\n",
       " 'associate',\n",
       " 'disassociated']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in vecsub(t, lookup[\"disassociate\"], lookup[\"diss\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound symbolism tinting\n",
    "\n",
    "Research in psychology and psycholinguistics says that \"kiki\" is a \"sharp\" word and \"bouba\" is a \"round\" word. Let's re-compose a text while trying to make it \"sharp\" (by adding the vector for `kiki` to each word) and \"round\" (by adding the vector for `babu` to each word). (Using `babu` instead of `bouba` because `bouba` isn't in CMUdict.)\n",
    "\n",
    "Here's Frost's famous \"The Road Not Taken\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "two roads diverged in a yellow wood\n",
    "and sorry i could not travel both\n",
    "and be one traveler long i stood\n",
    "and looked down one as far as i could\n",
    "to where it bent in the undergrowth\n",
    " \n",
    "then took the other as just as fair\n",
    "and having perhaps the better claim\n",
    "because it was grassy and wanted wear\n",
    "though as for that the passing there\n",
    "had worn them really about the same\n",
    " \n",
    "and both that morning equally lay\n",
    "in leaves no step had trodden black\n",
    "oh i kept the first for another day\n",
    "yet knowing how way leads on to way\n",
    "i doubted if i should ever come back\n",
    " \n",
    "i shall be telling this with a sigh\n",
    "somewhere ages and ages hence\n",
    "two roads diverged in a wood and i\n",
    "i took the one less traveled by\n",
    "and that has made all the difference\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewritten as a \"sharp\" poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kooky roads diverged in eh yellow woodke\n",
      "And sarti i gokey pecan keevil booth\n",
      "And pee one traveler long i stookey\n",
      "And loci down one as far as i gokey\n",
      "Tuckey waikiki eke beak in thi undergrowth\n",
      "\n",
      "Then kupek thi other as cheeky as fichera\n",
      "And having perhaps thi becky keim\n",
      "Picky eke was keesee and waikiki wacky\n",
      "Though as for peak thi peeking geeky\n",
      "Hakki worn them keeley kabuki thi safekeeping\n",
      "\n",
      "And booth peak morning kiki lay\n",
      "In teves know techie hakki teagarden blackie\n",
      "Oh i khaki thi thirsty for another ghee\n",
      "Fekete keown how way tiegs on tuckey way\n",
      "I tiki if i shooed keever come bacchi\n",
      "\n",
      "I kishi pee leckey keith withey eh psyche\n",
      "Squeaky keizai and keizai hence\n",
      "Kooky roads diverged in eh woodke and i\n",
      "I kupek thi one leckey keevil be\n",
      "And peak has pigue all thi defrance\n"
     ]
    }
   ],
   "source": [
    "for line in text.split(\"\\n\"):\n",
    "    print(' '.join([words[vecsum(t, lookup[word], lookup[\"kiki\"], 0.8)[0]] for word in line.split()]).capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as a \"round\" poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chubu roads barboursville in a yellow would\n",
      "And bari i koba knob travel both\n",
      "And bobby one bosler daum i stobaugh\n",
      "And jukebox bowed one as fahd as i koba\n",
      "To bowell it bondt in the bogard\n",
      "\n",
      "Babu bocook the bother as babu as fair\n",
      "And having perhaps the bobbette claim\n",
      "Boggess it zabawa barresi and wambaugh bowell\n",
      "Though as for bogacz the babu babu\n",
      "Hob worn them bodley abboud the same\n",
      "\n",
      "And both bogacz booming equally lay\n",
      "In bob's know bobbette hob bodden blob\n",
      "Oh i bobcat the first for bother doi\n",
      "Bobbette knowing baja way bob's on to way\n",
      "I bowed if i should ever kebab bob\n",
      "\n",
      "I shall bobby babu boggess with a seibu\n",
      "Babu mugabe's and mugabe's hence\n",
      "Chubu roads barboursville in a would and i\n",
      "I bocook the one babu traveled ba\n",
      "And bogacz has mugabe all the ballance\n"
     ]
    }
   ],
   "source": [
    "for line in text.split(\"\\n\"):\n",
    "    print(' '.join([words[vecsum(t, lookup[word], lookup[\"babu\"], 0.8)[0]] for word in line.split()]).capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And hey, just for fun, let's add the word \"road\" to the poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To roads road in a yellow would\n",
      "And sorrows ah could not devilwood boge\n",
      "And be rowen traveler long ah road\n",
      "And looked lowdown rowen a.s far a.s ah could\n",
      "To haywood it bent in the undergrowth\n",
      "\n",
      "Then rook the other a.s road a.s fair\n",
      "And having perhaps the eurodollar crowed\n",
      "Road it was grode and road haywood\n",
      "Zoh a.s for rhodus the road narrowed\n",
      "Hid worn them reload o'dowd the same\n",
      "\n",
      "And boge rhodus morning equally lay\n",
      "In roads know road hid roden brode\n",
      "Oh ah roadcap the first for another doi\n",
      "Road rowing doha way roads on to way\n",
      "Ah road if ah should ever come roadcap\n",
      "\n",
      "Ah shall be rodale road with a sigh\n",
      "Road loges and loges hence\n",
      "To roads road in a would and ah\n",
      "Ah rook the rowen road traveled lodi\n",
      "And rhodus has made all the durrance\n"
     ]
    }
   ],
   "source": [
    "for line in text.split(\"\\n\"):\n",
    "    print(' '.join([words[vecsum(t, lookup[word], lookup[\"road\"], 0.95)[0]] for word in line.split()]).capitalize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
